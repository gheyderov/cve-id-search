import requests
from io import BytesIO

from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from reportlab.lib import colors
from reportlab.lib.enums import TA_CENTER
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, KeepTogether
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle


styles = getSampleStyleSheet()
style_normal = styles["Normal"]
bold_style = ParagraphStyle(
    "Bold",
    parent=style_normal,
    fontName="Helvetica-Bold"
)


def create_pdf_from_urls(cve_code):
    buffer = BytesIO()
    doc = SimpleDocTemplate(
        buffer,
        pagesize=A4,
        title=f"{cve_code} | CVE Report",
        leftMargin=20,
        rightMargin=20,
        topMargin=20,
        bottomMargin=20
    )

    content = []

    # URLs
    urls = [
        f"https://nvd.nist.gov/vuln/detail/{cve_code}",
        f"https://www.exploit-db.com/search?cve={cve_code}",
        # f"https://cve.mitre.org/cgi-bin/cvename.cgi?name={cve_code}",
        # f"https://vulners.com/search?query={cve_code}",
        # f"https://vulmon.com/vulnerabilitydetails?qid={cve_code}"
    ]

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
    }

    print("="*50)
    for i, url in enumerate(urls):
        response = requests.get(url, headers=headers)

        print(f"URL: {url}")
        print(f"Status code: {response.status_code}")
        if response.status_code == 200:
            html_content = response.content
            soup = BeautifulSoup(html_content, "html.parser")

            if i > 0:
                content.append(Paragraph("-" * 50, style_normal))

            title = soup.title.string if soup.title else "Title not found"
            content.append(Paragraph(f"<b>Title:</b> {title}", bold_style))
            content.append(Paragraph(f"<b>URL:</b> {url}", bold_style))
            content.append(Spacer(1, 12))
            content.append(Paragraph("<b>Content:</b>", bold_style))
            content.append(Spacer(1, 6))

            if url == urls[0]:
                create_pdf_from_nist_url(soup, headers, content)
            elif url == urls[1]:
                create_pdf_from_exploitdb_url(url, content)

            content.append(Spacer(1, 20))
    print("="*50)

    doc.build(content)
    pdf_content = buffer.getvalue()
    buffer.close()
    return pdf_content


def create_pdf_from_nist_url(soup, headers, content):
    # ===== Description =====
    vuln_description = soup.find("h3", id="vulnDescriptionTitle")
    if vuln_description:
        description_text = vuln_description.get_text(strip=True)
        content.append(Paragraph(description_text, bold_style))

        description_paragraph = vuln_description.find_next_sibling(
            "p")
        if description_paragraph:
            description_text = description_paragraph.get_text(
                strip=True)
            content.append(
                Paragraph(description_text, style_normal))
        else:
            content.append(
                Paragraph("Description paragraph not found", style_normal))
    else:
        content.append(
            Paragraph("Description title not found", style_normal))
    content.append(Spacer(1, 6))

    # ===== Severity and Metrics =====
    nist_v3_metric = soup.find(
        "input", {"id": "nistV3MetricHidden"})
    if nist_v3_metric:
        value_content = nist_v3_metric["value"]
        value_soup = BeautifulSoup(value_content, "html.parser")
        vuln_cvssv3_score_container = value_soup.find(
            "p", {"data-testid": "vuln-cvssv3-score-container"})

        if vuln_cvssv3_score_container:
            vuln_cvssv3_title = value_soup.find(
                id="nistV3Metric").find("strong").get_text(strip=True)
            content.append(
                Paragraph(f"<b>{vuln_cvssv3_title}</b>", bold_style))

            # Extracting the data from the container
            for strong_tag in vuln_cvssv3_score_container.find_all("strong"):
                key = strong_tag.text.strip()

                # Get span tags and combine values
                sibling = strong_tag.find_next_sibling()
                values = ""
                while sibling:
                    if sibling.name == "span":
                        values += sibling.text.strip() + " "
                    else:
                        break
                    sibling = sibling.find_next_sibling()

                # Add key and value to PDF content
                content.append(
                    Paragraph(f"<b>{key}</b> {values.strip()}", style_normal))

            content.append(Spacer(1, 6))

    # ===== References Table =====
    references_table = soup.find(
        "table", {"data-testid": "vuln-hyperlinks-table"})
    if references_table:
        vendor_advisory_urls = []
        other_urls = []

        rows = references_table.find_all("tr")
        for row in rows:
            link_cell = row.find(
                "td", {"data-testid": lambda x: x and "vuln-hyperlinks-link" in x})
            res_type_cell = row.find(
                "td", {"data-testid": lambda x: x and "vuln-hyperlinks-resType" in x})

            if link_cell and res_type_cell:
                link = link_cell.text.strip()
                res_type = res_type_cell.text.strip()

                try:
                    ref_response = requests.head(
                        link, headers=headers)
                    if ref_response.status_code in [200, 301, 302, 303, 307, 308]:
                        if "Vendor Advisory" in res_type:
                            vendor_advisory_urls.append(link)
                        else:
                            other_urls.append(link)
                except:
                    pass

        if vendor_advisory_urls:
            content.append(
                Paragraph("<b>References (Vendor Advisory):</b>", bold_style))
            for url in vendor_advisory_urls:
                content.append(Paragraph(url, style_normal))

        if other_urls:
            content.append(
                Paragraph("<b>References (Other):</b>", bold_style))
            for url in other_urls:
                content.append(Paragraph(url, style_normal))


def create_pdf_from_exploitdb_url(url, content):
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-gpu")

        driver = webdriver.Chrome(options=chrome_options)

        driver.get(url)

        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located(
            (By.CSS_SELECTOR, "#exploits-table tbody tr td")))

        page_source = driver.page_source

        expoloits_soup = BeautifulSoup(page_source, "html.parser")

        exploits_table = expoloits_soup.find(
            "table", {"id": "exploits-table"})

        # ===== Exploits Table =====
        if exploits_table:
            if exploits_table.find("tbody") and not exploits_table.find("td", {"class": "dataTables_empty"}):
                print("----- exploits table -----")
                print(exploits_table)
                print("----- END exploits table -----")

                data_style = ParagraphStyle(
                    name="TableData",
                    parent=style_normal,
                    fontName="Helvetica",
                    fontSize=7.5,
                )
                table_rows = exploits_table.find_all("tr")
                table_data = []
                check_rows = []
                close_rows = []

                for row in table_rows:
                    row_data = []
                    has_header = False
                    has_check = False
                    has_close = False

                    for cell in row.find_all("th"):
                        has_header = True
                        row_data.append(Paragraph(cell.text.strip(), style=ParagraphStyle(
                            name="TableHeader",
                            fontName="Helvetica-Bold",
                            fontSize=9,
                            alignment=TA_CENTER
                        )))

                    for cell in row.find_all("td"):
                        if cell.find("i", {"class": lambda x: x and "mdi mdi-check" in x}):
                            print("exploit row check")
                            has_check = True
                            row_data.append(
                                Paragraph(u"\u2714", data_style))
                        elif cell.find("i", {"class": lambda x: x and "mdi mdi-close" in x}):
                            print("exploit row close")
                            has_close = True
                            row_data.append(
                                Paragraph(u"\u2716", data_style))
                        elif cell.find("a") and cell.find("a").find("i", {"class": lambda x: x and "mdi mdi-download" in x}):
                            link_text = "<a href='{}'>{}</a>".format(
                                "https://www.exploit-db.com" + cell.find("a")["href"], u"\u2193")
                            row_data.append(Paragraph(link_text, style=ParagraphStyle(
                                name="DataDownload",
                                fontSize=11,
                                alignment=TA_CENTER
                            )))
                        elif cell.find("a"):
                            link_text = "<a href='{}'>{}</a>".format(
                                "https://www.exploit-db.com" + cell.find("a")["href"], cell.text.strip())
                            row_data.append(
                                Paragraph(link_text, data_style))
                        else:
                            row_data.append(
                                Paragraph(cell.text.strip(), data_style))

                    if has_header:
                        table_data.append(row_data)  # Append header row
                    elif has_check:
                        check_rows.append(row_data)
                    elif has_close:
                        close_rows.append(row_data)

                # Append check rows
                for r_data in check_rows:
                    table_data.append(r_data)

                # Append close rows
                for r_data in close_rows:
                    table_data.append(r_data)

                pdf_exploits_table = Table(table_data, repeatRows=1)

                pdf_exploits_table.setStyle(TableStyle([
                    ("GRID", (0, 0), (-1, -1), 1, colors.black),
                    ("BACKGROUND", (0, 0), (-1, 0), colors.grey),
                    ("INNERGRID", (0, 0), (-1, -1), 0.25, colors.black),
                    ("BOX", (0, 0), (-1, -1), 0.25, colors.black),
                    ("ALIGN", (0, 0), (-1, -1), "LEFT"),
                    ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                    ("FONTSIZE", (0, 0), (-1, -1), 7.5),
                    ("TEXTCOLOR", (0, 0), (-1, -1), colors.black),
                ]))

                content.append(pdf_exploits_table)
            elif exploits_table.find("tbody") and exploits_table.find("td", {"class": "dataTables_empty"}):
                content.append(
                    Paragraph("No data available in exploits table", style_normal))
    except Exception as e:
        print(f"ERROR: {str(e)}")
    finally:
        if "driver" in locals():
            driver.quit()
